#' parse_all_vr
#'
#' @param dir_vr A character string identifying the directory where verification reports are held
#' @param target_account The account to be parsed
#' @param target_month The month of verification reports to be parsed
#'
#' @return list(filepaths, # of rows)
#'         This function reads/cleans/writes parsed verification files to a db for later use. The function
#'         is designed to process the verification reports of each month of each account. Nested functions include:
#'         parse_detail(), parse_summary(), parse_refresh()
#'
#' @importFrom  plyr rbind.fill
#' @export
parse_all_vr <- function(dir_vr,
                         target_account,
                         target_month) {
  ### get all VR files associated with this target month and account
  files = grep(paste(target_account,target_month,sep ="_"),dir(dir_vr), value = T)
  usage_detail = data.frame()

  # file = "cashbval_usage_detail_30171310_201810"
  for (file in files) {
    filepath <- file.path(dir_vr,file)

    ### if usage detail file of ad hoc , scheduled or cashbval, send to parse_detail()
    if (grepl("usage_detail|scheduled_securities|cashbval_usage_detail", filepath)) {
      this_usage <- parse_detail(filepath)
      usage_detail = plyr::rbind.fill(usage_detail, this_usage)
    }
  }

  return(usage_detail)
}

### ----------------------------------------------------------------------------

#' parse_detail
#'
#' @param filepath A character string giving the filepath to be parsed
#'
#' @return dataframe containing the parsed results
#'         This function takes a given file and reads/clean/write the contents for later analysis.
#'         This function contains the following sub-functions:
#'                 check_db() - checks the database to see if the file has been processed already
#'                 YNtoTF() - converts Y/N entries to T/F for better performance and numerical processing
#'                 cluster_times() - clusters rows of the verification reports by their date/account
#'                 product_split()
#' @import magrittr
#' @import tibble
#' @import readr
#' @import dplyr
#' @export
parse_detail <- function(filepath) {
  stopifnot(file.exists(filepath))
  this_usage = data.frame()

  ### read the usage detail file
  if (grepl("usage_detail|scheduled_securities", filepath) &
      !grepl("_usage_detail", filepath)) {
    this_usage <- readr::read_delim(
      filepath,
      delim = "|",
      na = "NA",
      # col_types = 'ffffcfcfcccccccc', ### sets the col types but commented out because super slow
      col_types = readr::cols(.default = "c"),
      trim_ws = T
    )
  } else if(grepl("cashbval_usage_detail", filepath)) {
    this_usage <- readr::read_delim(
      filepath,
      delim = "|",
      na = "NA",
      col_types = readr::cols(.default = "c"),
      trim_ws = T
    )

    this_usage = this_usage %>% rename(BILLABLE_PRODUCT = MATERIAL_DESC)
    x = duplicated(this_usage[,c('BILLABLE_PRODUCT','ID_BB_UNIQUE')])
    table(x)
    this_usage = this_usage %>%
      mutate(CTRB_AC = ifelse(x, "N", "Y"))

    this_usage = this_usage %>%
      mutate(DOWNLOAD_PRODUCT = BILLABLE_PRODUCT, CTRB_BILLING = "Y", CTRB_UNIQ = "N",
             CTRB_MAIN = "N", INIT_CHARGE = "N", CTRB_PD = "N", BO_OPTIMIZED = "N")
    table(this_usage$BILLABLE_PRODUCT,this_usage$CTRB_AC)

    # table(
    #   this_usage %>% distinct(ID_BB_UNIQUE, BILLABLE_PRODUCT) %>% pull(BILLABLE_PRODUCT),
    #   this_usage %>% distinct(ID_BB_UNIQUE, BILLABLE_PRODUCT, .keep_all = T) %>% pull(SNAPSHOT)
    # )
  } else {
    return(this_usage)
  }

  this_usage %<>% dplyr::mutate_at(dplyr::vars(dplyr::matches("^CTRB|INIT|BO_OPT")), dplyr::funs(. == "Y"))
  this_usage %<>% dplyr::mutate(tz = substr(this_usage$PROCESSED_TIME, 21, 23))
  # table(this_usage$tz)

  this_usage$PROCESSED_TIME <- as.POSIXct(paste0(
    substr(this_usage$PROCESSED_TIME, 1, 20),
    substr(this_usage$PROCESSED_TIME, 24, 28)
  ), format = "%c")
  ### BST timezone is not allowed, I've not checked if different timezones are used in various places

  ### if there are zero rows, then return the empty dataframe
  if (nrow(this_usage) == 0) {
    return(this_usage)
  }

  # this_usage$PRODUCT_TYPE %>% unique()

  ### special treatment for scheduled usage detail
  if (grepl("scheduled_securities", filepath)) {
    this_usage$PRODUCT_TYPE <- "Sched"
  }

  ### special treatment for ad-hoc usage detail
  if (grepl("usage_detail", filepath) &
      !grepl("_usage_detail", filepath)) {
    this_usage$PRODUCT_TYPE <- "Adhoc"
    names(this_usage) <-
      gsub("QUALIFER", "QUALIFIER", names(this_usage))

    if (!this_usage %>% tibble::has_name("CTRB_BAC")) {
      this_usage %<>% mutate(CTRB_BAC = BO_OPTIMIZED * CTRB_BILLING)
    }

    # table(is.na(this_usage$QUALIFIER))
  }

  ### special treatment for BVAL usage detail, excluded for now

  # if(grepl('cashbval_usage_detail', filepath)) {
  #   names(this_usage) = gsub('MATERIAL_DESC', 'BILLABLE_PRODUCT', names(this_usage)) this_usage$QUALIFIER = ' '
  # }

  # this_usage$BILLABLE_PRODUCT <- as.character(this_usage$BILLABLE_PRODUCT)
  return(this_usage)
}

### ----------------------------------------------------------------------------

#' parse_summary
#'
#' @param conn A character string identifying the database location
#' @param filepath A character string giving the filepath to be parsed
#'
#' @return dataframe containing the parsed results
#'         This function parses the summary verification reports for later comparison to the synthetic invoice derived
#'         in parse_detail. This function contains the following sub-functions:
#'                 check_db() - checks the database to see if the file has been processed already
#'
#' @export
parse_summary <- function(conn, filepath) {
  return(T)
}

### ----------------------------------------------------------------------------

# parse_refresh <- function(conn, filepath) { }

### ----------------------------------------------------------------------------

# check_db <- function(conn) {
#   return(T)
# }

### ----------------------------------------------------------------------------

#' cluster_times
#'
#' @param this_usage data.frame from parse_detail()
#' @param cluster logical. Defaults to F.
#'
#' @return This function takes the date/time and account of each row and clusters those rows with matching info
#'         together to be treated as part of a single request event.
#'
#' @importFrom  stats aggregate
#'
#' @export
cluster_times = function(this_usage, cluster = F, time = .5) {

  ### find all output files that occur over multiple days and cluster them into 60 second groups
  outs = aggregate(this_usage$PROCESSED_TIME,
                   by = list(OUTPUT_ID = this_usage$OUTPUT_ID),
                   FUN = function(x) {difftime(range(x)[2], range(x)[1], units = "hours")})

  outs = outs[outs$x > time,]

  data = data.frame()

  if(nrow(outs) > 0) {
    i = 1
    if(cluster == F) {
      for(i in 1:nrow(outs)) {
        single_output = this_usage[this_usage$OUTPUT_ID == outs[i,"OUTPUT_ID"],]
        # plot(single_output$PROCESSED_TIME)
        print(outs[i,"OUTPUT_ID"])
        print(length(single_output$PROCESSED_TIME))

        ### no more fancy h-cluster :(
        # # Ward Hierarchical Clustering
        # d <- dist(single_output$PROCESSED_TIME, method = "euclidean") # distance matrix
        # fit <- hclust(d, method="ward")
        # # plot(fit) # display dendogram
        # groups <- cutree(fit, h = 10) # cut tree at 10 second mark
        # # draw dendogram with red borders around the 5 clusters
        # # rect.hclust(fit, k=5, border="red")

        groups = single_output$PROCESSED_TIME

        print(table(groups))
        print(paste0(i," of ",nrow(outs)))
        single_output$OUTPUT_ID_cluster = paste0(single_output$OUTPUT_ID," (", groups,")")

        data = plyr::rbind.fill(data,single_output)
      }
    }

    ### incomplete, F cluster option just using unique PROCESSED TIMES

    if(cluster == T) {
      for(i in 1:nrow(outs)) {
        single_output = this_usage[this_usage$OUTPUT_ID == outs[i,"OUTPUT_ID"],]
        # plot(single_output$PROCESSED_TIME)
        print(outs[i,"OUTPUT_ID"])
        print(length(single_output$PROCESSED_TIME))
        print(length(unique(single_output$PROCESSED_TIME)))

        ### build groups just by unique times
        single_reqs = split(single_output,single_output$PROCESSED_TIME)
        for (j in 1:length(single_reqs)) {
          single_reqs[[j]]$OUTPUT_ID_cluster = paste0(single_reqs[[j]]$OUTPUT_ID," (", j,")")
        }

        gc()
        single_output = unsplit(single_reqs, single_output$PROCESSED_TIME, drop = FALSE)

        data = plyr::rbind.fill(data,single_output)
        print(table(single_output$OUTPUT_ID_cluster))
        print(paste0(i," of ",nrow(outs)))
      }
    }
  }


  ### add back all the non-repeating outs
  outs = aggregate(this_usage$PROCESSED_TIME,
                   by = list(OUTPUT_ID = this_usage$OUTPUT_ID),
                   FUN = function(x) {difftime(range(x)[2], range(x)[1], units = "hours")})

  outs = outs[outs$x <= time,]
  single_output = this_usage[this_usage$OUTPUT_ID %in% outs$OUTPUT_ID,]
  single_output$OUTPUT_ID_cluster = single_output$OUTPUT_ID

  data = plyr::rbind.fill(data,single_output)
  length(unique(data$OUTPUT_ID_cluster))
  length(unique(data$OUTPUT_ID))

  ### now i've seperated all the requests that went out with the same output ID, this will help me match to .req files

  ### Let's find the largest requests and cross them against DL Categroies

  ### this shows me all 3000 .out files and the distribution of DL categories asked for in each
  names(data)
  # write.csv(table(data$OUTPUT_ID_cluster,data$BILLABLE_PRODUCT)[,order(table(data$BILLABLE_PRODUCT), decreasing = T)], file = "dusseldorf_reqs.csv")

  return(data)
}

### ----------------------------------------------------------------------------

# product_split <- function(filepath) {
#   return(T)
# }

### ----------------------------------------------------------------------------

#' Get all VR files from designated directory
#'
#' @param dir_vr the directory where we expect to find verification reports
#'
#' @return data.frame describing all verification reports found and thier attributes
#'
#' @examples \dontrun{
#'   dir_vr %>% get_vrs()
#'   }
#'
#' @export
get_vrs = function(dir_vr) {
  vrs = list.files(dir_vr)
  vrs = vrs[!(vrs %in% "downloaded")]

  ### removes a wonky named partial file
  vrs = vrs[nchar(vrs) < 50]
  # table(nchar(vrs))

  ### get inventory of all given vr files

  vrs_summary = data.frame(filename = vrs)

  vrs_summary$month = sapply(strsplit(vrs,"_"), function(x) x[length(x)])
  vrs_summary$account = sapply(strsplit(vrs,"_"), function(x) x[length(x)-1])

  vrs_files_type = sapply(strsplit(vrs,"_"), function(x) x[1:(length(x)-2)])
  vrs_summary$file_type = sapply(vrs_files_type, function(x) paste0(x,collapse = "_"))

  vrs_summary$file_path = file.path(dir_vr,vrs)
  vrs_summary$file_size = file.info(vrs_summary$file_path)[,1]

  # head(vrs_summary)
  length(unique(vrs_summary$account))

  vrs_summary = vrs_summary[order(vrs_summary[,"account"], vrs_summary[,"month"]), ]
  row.names(vrs_summary) = 1:nrow(vrs_summary)
  return(vrs_summary)
}

#' Write the parsed VR detail file contents to DB
#'
#' @param this_usage dataframe of the parsed VR detail
#' @param conn DB connection SQLite object
#' @param args_id client_id as it currently appears in the DB
#' @param target_account account number for this Bloomberg account
#' @param target_month billing month in 'MMYYYY'
#' @param drop logical if T then existing table will be dropped
#'
#' @import RSQLite
#'
#' @return dataframe representing the whole table just written to DB
#' @export
write_vr_detail = function(this_usage, conn, args_id, target_account, target_month, drop = F) {
  ### write the usage table as well. Have fun exploding the DB file!

  df = this_usage

  ### cost is no longer part of this table
  # df = df[,1:which(names(df) %in% "cost")]

  if (!("CTRB_BAC" %in% names(df))) {
    df$CTRB_BAC = NA
  }
  df$client_id = args_id
  df$account = target_account
  df$month = target_month
  df$PROCESSED_TIME = as.character(df$PROCESSED_TIME)
  df$id = paste0(sprintf("%03d", args_id),
                 sprintf("%09d", as.numeric(target_account)),
                 target_month,
                 sprintf("%09d", 1:nrow(df)))

  # names(df)[!(names(df) %in% dbListFields(conn,"vr_detail"))]
  # ?which

  if(drop == T & ("vr_detail" %in% RSQLite::dbListTables(conn))) {
    dbExecute(conn,'DROP TABLE "vr_detail"')
  }

  if(!("vr_detail" %in% RSQLite::dbListTables(conn))) {
    RSQLite::dbWriteTable(conn, "vr_detail", df)
  } else {
    RSQLite::dbExecute(conn, paste0('DELETE FROM vr_detail WHERE "client_id" = ',args_id,
                                    ' AND "account" = ',target_account,
                                    ' AND "month" = ',target_month))

    RSQLite::dbWriteTable(conn, "vr_detail", df, append = T)

  }

  RSQLite::dbGetQuery(conn, paste0('SELECT * FROM vr_detail WHERE "client_id" = ',args_id,
                                   ' AND "account" = ',target_account,
                                   ' AND "month" = ',target_month))

  # dbGetQuery(conn, paste0('SELECT * FROM vr_detail WHERE "client_id" = ',args_id))
  # return(df)
}


#' Assign the cost of each hit according to a rate card
#'
#' @param this_usage data.frame of hits as read from DB
#' @param rate_card any rate card with relevant rates in effective_rate or List.Price column
#' @param run integer, gives a run count to differentiate different iterations
#' @param conn DB connection
#'
#' @return dataframe of the hits joined with vr_detail table. data to create virtual invoice also include
#' @export
assign_vr_cost = function(this_usage, rate_card, conn, run = 1) {
  # t = this_usage
  # this_usage = t
  # rate_card = list_rate_card
  # run = 1

  rate_card$Product = trimws(gsub("\\s*\\([^\\)]+\\)","",rate_card$Product))
  rate_card$Product = trimws(gsub("Unique|Access","",rate_card$Product))

  names(rate_card) = gsub("Product","BILLABLE_PRODUCT", names(rate_card))
  unique(rate_card$Request.Type)
  rate_card$INIT_CHARGE = sapply(rate_card$Request.Type, function(x) if(!is.na(x) & x %in% "Unique") T else F)
  rate_card$CTRB_MAIN = sapply(rate_card$Request.Type, function(x) if(!is.na(x) & x %in% "Maintained") T else F)
  rate_card$BO_OPTIMIZED = sapply(rate_card$Request.Type, function(x) if(!is.na(x) & x %in% c("Access Bulk","BoOptimized Access")) T else F)

  ### rate card add an extra for the maintained to accept UNIQ = T and MAIN = T
  rate_card = rate_card %>% filter(Request.Type %in% "Maintained") %>% mutate(INIT_CHARGE = T) %>%
    plyr::rbind.fill(rate_card)

  ### rate card add an extra for the bo access to accept UNIQ = T and BO_OPT = T
  rate_card = rate_card %>% filter(Request.Type %in% c("Access Bulk","BoOptimized Access")) %>% mutate(INIT_CHARGE = T) %>%
    plyr::rbind.fill(rate_card)

  # list_rate_card$Request.Type %>% unique()
  # head(rate_card)

  ### manual substitutions to match rate card derived from invoice to BILLABLE_PRODUCT from VR reports
  rate_card$BILLABLE_PRODUCT = gsub("EqEq","EqtyEqty",rate_card$BILLABLE_PRODUCT)
  rate_card$BILLABLE_PRODUCT = gsub("SovSupAg ","SovSupAgny ",rate_card$BILLABLE_PRODUCT)
  unique(rate_card$BILLABLE_PRODUCT)

  rate_card = rate_card %>% mutate(
    List.Price = ifelse(
      Fee.Type %in% "SCHEDULED" &
        Request.Type %in% "Unique",
      effective_rate,
      List.Price
    )
  )

  rate_card = rate_card[!duplicated(rate_card[,c("BILLABLE_PRODUCT","INIT_CHARGE","CTRB_MAIN","BO_OPTIMIZED")]),]
  rate_card

  ######################################################
  ### merge with this usage and produce cost ###########

  ### a quick column clean up before the merge just in case
  this_usage = this_usage[,!(names(this_usage) %in% c("q_effective_rate","Request.Type","Request.Type.x","Request.Type.y","effective_rate"))]
  # head(this_usage)
  # head(rate_card)

  table(unique(this_usage$BILLABLE_PRODUCT) %in% unique(rate_card$BILLABLE_PRODUCT))
  table(this_usage$BILLABLE_PRODUCT, is.na(this_usage$CTRB_MAIN))

  this_usage = this_usage %>% mutate(CTRB_MAIN = ifelse(is.na(CTRB_MAIN),F,CTRB_MAIN))

  ### the scheduled securities VR's don't have an "INIT_CHRG" column. I will add one for now
  ### the real INIT_CHRG will need to be calculated from the bands, I don't have this yet
  table(this_usage$BILLABLE_PRODUCT, is.na(this_usage$INIT_CHARGE))
  if(is.null(this_usage$INIT_CHARGE)) {
    this_usage$INIT_CHARGE = NA
  }
  # this_usage[is.na(this_usage$INIT_CHARGE),"INIT_CHARGE"] = F

  if(is.null(this_usage$CTRB_MAIN)) {
    this_usage$CTRB_MAIN = F
  }
  # this_usage[is.na(this_usage$CTRB_MAIN),"CTRB_MAIN"] = F
  # table(this_usage$CTRB_MAIN)
  # this_usage = this_usage %>% mutate(INIT_CHARGE = ifelse(CTRB_MAIN %in% T, F, INIT_CHARGE))

  ### add T for INIT_CHARGE if !duplicated
  table(this_usage$BILLABLE_PRODUCT, this_usage$INIT_CHARGE, useNA = "ifany")

  df = this_usage %>% filter(is.na(INIT_CHARGE))
  x = duplicated(df[,c("ID_BB_UNIQUE","BILLABLE_PRODUCT")])
  df = df %>% mutate(INIT_CHARGE = ifelse(x,F,T))
  this_usage = this_usage %>% filter(!is.na(INIT_CHARGE)) %>% plyr::rbind.fill(df) %>%
    mutate(CTRB_MAIN = F)

  this_usage = merge(this_usage,rate_card, by = c("BILLABLE_PRODUCT","INIT_CHARGE", "CTRB_MAIN", "BO_OPTIMIZED"), all.x = T)
  table(this_usage$BILLABLE_PRODUCT, this_usage$Request.Type, useNA = "ifany")

  this_usage = this_usage %>% mutate(
    # List.Price = ifelse(is.na(List.Price), .01, List.Price),
    cost = List.Price
    )
  # if("effective_rate" %in% names(this_usage)) {
  #   this_usage$cost = this_usage$effective_rate
  # } else if("List.Price" %in% names(this_usage)) {
    # this_usage$cost = this_usage$List.Price
  # }

  # head(this_usage)

  ### checking the ones where merge failed to bring a q_effective_rate
  # table(unique(this_usage$BILLABLE_PRODUCT) %in% unique(rate_card$BILLABLE_PRODUCT))
  print("any missing BILLABLE_PRODUCT from invoice rate card")
  print(table(this_usage[is.na(this_usage$cost),"BILLABLE_PRODUCT"],this_usage[is.na(this_usage$cost),"INIT_CHARGE"]))

  ########################################################################

  ### man, i got a lot shit missing, let's look at the product details sheet again
  ### how can there be so many items that don't appear on the invoice?
  ### for now it can take placeholder values, but wll need to come back to this

  # product_details$Product
  # this_usage$BILLABLE_PRODUCT
  unique(this_usage$BILLABLE_PRODUCT)
  # this_usage$BILLABLE_PRODUCT %in% product_details$Product

  #################################################################################

  ### default, if no rate exists on rate card, go with $.01 and $1 for access and unique
  this_usage$cost[this_usage$CTRB_BILLING == F] = 0

  ### this seems key, it must have an init or access charge...
  this_usage$cost[this_usage$INIT_CHARGE == F & this_usage$CTRB_AC == F] = 0

  table(is.na(this_usage$cost))

  this_usage[is.na(this_usage$cost) & this_usage$INIT_CHARGE == T,"cost"] = 1
  this_usage[is.na(this_usage$cost) & this_usage$INIT_CHARGE == F,"cost"] = .01
  this_usage$cost = round(this_usage$cost, 4)

  # table(this_usage$INIT_CHARGE)

  ### now I'm inspecting my totals, initially added up to 1.9 million so seems wrong
  sum(this_usage$cost, na.rm = T)
  table(is.na(this_usage$cost))

  # vr_invoice_summary[vr_invoice_summary$account %in% target_account &
  #                      vr_invoice_summary$month %in% target_month, "vr_detail_amount"] = sum(this_usage$cost, na.rm = T)
  # vr_invoice_summary

  # table(this_usage$BILLABLE_PRODUCT, this_usage$CTRB_UNIQ)
  table(this_usage$BILLABLE_PRODUCT, this_usage$INIT_CHARGE)

  aggregate(cost ~  INIT_CHARGE + BILLABLE_PRODUCT, data = this_usage, FUN = sum)

  # ?sum
  # table(this_usage$cost,this_usage$CTRB_AC)
  # table(this_usage$q_effective_rate)

  # sum(as.numeric(this.product_details$Quantity))

  # temp = this_usage[this_usage$cost == 1.7,]


  this_usage = this_usage %>% as_tibble() %>%
    dplyr::select(
      vr_detail_id = id,
      BILLABLE_PRODUCT,
      INIT_CHARGE,
      CTRB_BILLING,
      CTRB_MAIN,
      BO_OPTIMIZED,
      Request.Type,
      Data.Category,
      Fee.Type,
      Asset.Type,
      cost
    ) %>%
    dplyr::mutate(run = paste0(substr(as.character(this_usage[1, "id"]), 1, 3), sprintf("%03d", run)))

  write_vr_cost(this_usage, conn, drop = F)
  # return(this_usage)
}

#' Write the parsed VR detail file contents to DB
#'
#' @param this_usage dataframe of the parsed VR detail costs
#' @param conn DB connection SQLite object
#' @param drop logical if T then existing table will be dropped
#'
#' @import RSQLite
#'
#' @return dataframe representing the whole table just written to DB
#' @export
write_vr_cost = function(this_usage, conn, drop = F) {
  ### write the usage table as well. Have fun exploding the DB file!

  df = this_usage
  run = as.character(this_usage[1,"run"])

  if(drop == T & ("vr_cost_detail" %in% RSQLite::dbListTables(conn))) {
    dbExecute(conn,'DROP TABLE "vr_cost_detail"')
  }

  if(!("vr_cost_detail" %in% RSQLite::dbListTables(conn))) {
    RSQLite::dbWriteTable(conn, "vr_cost_detail", df)
  } else {
    ### if the fields have changed, drop the old table and write a new one

    if(!all(names(this_usage) %in% dbListFields(conn,"vr_cost_detail"))) {
      dbExecute(conn,'DROP TABLE "vr_cost_detail"')
      RSQLite::dbWriteTable(conn, "vr_cost_detail", df)
    } else {
      RSQLite::dbExecute(conn, paste0('DELETE FROM vr_cost_detail WHERE "run" LIKE "',run,'"'))

      RSQLite::dbWriteTable(conn, "vr_cost_detail", df, append = T)
    }
  }


  RSQLite::dbGetQuery(conn, paste0('SELECT * FROM vr_cost_detail WHERE "run" LIKE "',run,'"'))

}
